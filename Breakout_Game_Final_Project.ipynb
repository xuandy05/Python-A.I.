{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Breakout Game Final Project",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xuandy05/Python-A.I./blob/master/Breakout_Game_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1pSswDUL9Nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install pyglet==1.3.2 > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-78q-9gMFpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import clear_output\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZU_9bF1MI81",
        "colab_type": "code",
        "outputId": "fd3d6fb5-15d6-4d51-9cdb-8ab0acafbd08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1013'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1013'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc3UlQEnKltE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_grayscale(img):\n",
        "    return np.mean(img, axis=2).astype(np.uint8)\n",
        "\n",
        "def downsample(img):\n",
        "    return img[::2, ::2]\n",
        "\n",
        "def preprocess(img):\n",
        "    return to_grayscale(downsample(img))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyhMkY_7RTb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_reward(reward):\n",
        "        return np.sign(reward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBQWjP5U-QB6",
        "colab_type": "code",
        "outputId": "6b9416d8-93cb-4029-e87c-15b4840653d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "\n",
        "def fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal):\n",
        "    \"\"\"Do one deep Q learning iteration.\n",
        "    Params:\n",
        "    - model: The DQN\n",
        "    - gamma: Discount factor (should be 0.99)\n",
        "    - start_states: numpy array of starting states\n",
        "    - actions: numpy array of one-hot encoded actions corresponding to the start states\n",
        "    - rewards: numpy array of rewards corresponding to the start states and actions\n",
        "    - next_states: numpy array of the resulting states corresponding to the start states and actions\n",
        "    - is_terminal: numpy boolean array of whether the resulting state is terminal\n",
        "    \n",
        "    \"\"\"\n",
        "    # First, predict the Q values of the next states. Note how we are passing ones as the mask.\n",
        "    next_Q_values = model.predict([next_states, np.ones(actions.shape)])\n",
        "    \n",
        "    # The Q values of the terminal states is 0 by definition, so override them\n",
        "    next_Q_values[is_terminal] = 0\n",
        "    # The Q values of each start state is the reward + gamma * the max next state Q value\n",
        "    Q_values = rewards + gamma * np.max(next_Q_values, axis=1)\n",
        "    target = actions * Q_values[:, None]\n",
        "    target = target[0]\n",
        "    # Fit the keras model. Note how we are passing the actions as the mask and multiplying\n",
        "    # the targets by the actions.\n",
        "    model.fit(\n",
        "        [start_states, actions], target,\n",
        "        epochs=1, batch_size=len(start_states), verbose=0\n",
        "    )\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wXYFj4D-oIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def atari_model(n_actions):\n",
        "    # We assume a theano backend here, so the \"channels\" are first.\n",
        "    ATARI_SHAPE = (105, 80, 4)\n",
        "\n",
        "    # With the functional API we need to define the inputs.\n",
        "    frames_input = keras.layers.Input(ATARI_SHAPE, name='frames')\n",
        "    actions_input = keras.layers.Input((n_actions,), name='mask')\n",
        "\n",
        "    # Assuming that the input frames are still encoded from 0 to 255. Transforming to [0, 1].\n",
        "    normalized = keras.layers.Lambda(lambda x: x / 255.0)(frames_input)\n",
        "    \n",
        "    # \"The first hidden layer convolves 16 8×8 filters with stride 4 with the input image and applies a rectifier nonlinearity.\"\n",
        "    conv_1 = keras.layers.convolutional.Convolution2D(\n",
        "        16, 8, 8, subsample=(4, 4), activation='relu'\n",
        "    )(normalized)\n",
        "    # \"The second hidden layer convolves 32 4×4 filters with stride 2, again followed by a rectifier nonlinearity.\"\n",
        "    conv_2 = keras.layers.convolutional.Convolution2D(\n",
        "        32, 4, 4, subsample=(2, 2), activation='relu'\n",
        "    )(conv_1)\n",
        "    # Flattening the second convolutional layer.\n",
        "    conv_flattened = keras.layers.core.Flatten()(conv_2)\n",
        "    # \"The final hidden layer is fully-connected and consists of 256 rectifier units.\"\n",
        "    hidden = keras.layers.Dense(256, activation='relu')(conv_flattened)\n",
        "    # \"The output layer is a fully-connected linear layer with a single output for each valid action.\"\n",
        "    output = keras.layers.Dense(n_actions)(hidden)\n",
        "    # Finally, we multiply the output by the mask!\n",
        "    filtered_output = keras.layers.multiply([output, actions_input])\n",
        "\n",
        "    model = keras.models.Model(input=[frames_input, actions_input], output=filtered_output)\n",
        "    optimizer = optimizer=keras.optimizers.RMSprop(lr=0.00025, rho=0.95, epsilon=0.01)\n",
        "    model.compile(optimizer, loss='mse')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6NOq1ZYPXv8",
        "colab_type": "code",
        "outputId": "2b441a69-05f3-4471-9908-44cae535713b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from collections import deque\n",
        "# Import the gym module\n",
        "import gym\n",
        "env = gym.make('BreakoutDeterministic-v4')\n",
        "\n",
        "memory = deque(maxlen = 1000000)\n",
        "gamma = 0.99\n",
        "alpha = 0.00025\n",
        "epsilon = 1\n",
        "epochs = 150\n",
        "model = atari_model(4)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "for i in range(epochs):\n",
        "  state = env.reset()\n",
        "  state = preprocess(state)\n",
        "  start_states = deque(maxlen = 4)\n",
        "  actions = deque(maxlen = 4)\n",
        "  next_states = deque(maxlen = 4)\n",
        "  rewards = deque(maxlen = 4)\n",
        "  is_terminal = deque(maxlen = 4)\n",
        "  done = False\n",
        "  total_reward = 0\n",
        "  \n",
        "  while not done:\n",
        "    e = random.uniform(0,1)\n",
        "    if e < epsilon or len(start_states)<4:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      not_states = np.reshape(np.array(start_states),[1,105,80,4])\n",
        "      preds = model.predict([not_states, np.ones([1,4])])\n",
        "      action = np.argmax(preds)\n",
        "    \n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = preprocess(next_state)\n",
        "    state = next_state\n",
        "    start_states.append(state)\n",
        "    actions.append(action)\n",
        "    next_states.append(next_state)\n",
        "    reward = transform_reward(reward)\n",
        "    rewards.append(reward)\n",
        "    is_terminal.append(done)\n",
        "    if len(actions) == 4:\n",
        "      data = np.array(actions)\n",
        "      encoded = to_categorical(data, num_classes=4)\n",
        "      encoded = np.amax(encoded,axis=0)\n",
        "      encoded = np.reshape(encoded,[1,4])\n",
        "      not_state = np.reshape(np.array(start_states),[1,105,80,4])\n",
        "      not_next_state = np.reshape(np.array(next_states),[1,105,80,4])\n",
        "      not_is_terminal = np.reshape(is_terminal,[1,4])\n",
        "      not_rewards = np.reshape(rewards,[1,4])\n",
        "      memory.append(((not_state),(encoded),np.array(not_rewards),(not_next_state),np.array(not_is_terminal)))\n",
        "    total_reward += reward\n",
        "  print(\"Epoch:\", i, \"Score:\", total_reward)\n",
        "  epsilon = min(0.1, epsilon-0.001) \n",
        "  batchsize = 500\n",
        "  if len(memory) > batchsize:\n",
        "    batch = random.sample(memory, batchsize)\n",
        "    for start_states, actions, rewards, next_states, is_terminal in batch:\n",
        "      fit_batch(model, gamma, start_states, actions, rewards, next_states, is_terminal)\n",
        "env.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (8, 8), activation=\"relu\", strides=(4, 4))`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (4, 4), activation=\"relu\", strides=(2, 2))`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=Tensor(\"mu...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Score: 1.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "Epoch: 1 Score: 0.0\n",
            "Epoch: 2 Score: 0.0\n",
            "Epoch: 3 Score: 5.0\n",
            "Epoch: 4 Score: 0.0\n",
            "Epoch: 5 Score: 0.0\n",
            "Epoch: 6 Score: 1.0\n",
            "Epoch: 7 Score: 5.0\n",
            "Epoch: 8 Score: 0.0\n",
            "Epoch: 9 Score: 5.0\n",
            "Epoch: 10 Score: 0.0\n",
            "Epoch: 11 Score: 0.0\n",
            "Epoch: 12 Score: 0.0\n",
            "Epoch: 13 Score: 5.0\n",
            "Epoch: 14 Score: 1.0\n",
            "Epoch: 15 Score: 0.0\n",
            "Epoch: 16 Score: 5.0\n",
            "Epoch: 17 Score: 0.0\n",
            "Epoch: 18 Score: 0.0\n",
            "Epoch: 19 Score: 0.0\n",
            "Epoch: 20 Score: 0.0\n",
            "Epoch: 21 Score: 0.0\n",
            "Epoch: 22 Score: 0.0\n",
            "Epoch: 23 Score: 2.0\n",
            "Epoch: 24 Score: 0.0\n",
            "Epoch: 25 Score: 0.0\n",
            "Epoch: 26 Score: 5.0\n",
            "Epoch: 27 Score: 5.0\n",
            "Epoch: 28 Score: 5.0\n",
            "Epoch: 29 Score: 0.0\n",
            "Epoch: 30 Score: 0.0\n",
            "Epoch: 31 Score: 0.0\n",
            "Epoch: 32 Score: 2.0\n",
            "Epoch: 33 Score: 5.0\n",
            "Epoch: 34 Score: 4.0\n",
            "Epoch: 35 Score: 0.0\n",
            "Epoch: 36 Score: 5.0\n",
            "Epoch: 37 Score: 0.0\n",
            "Epoch: 38 Score: 6.0\n",
            "Epoch: 39 Score: 0.0\n",
            "Epoch: 40 Score: 0.0\n",
            "Epoch: 41 Score: 0.0\n",
            "Epoch: 42 Score: 0.0\n",
            "Epoch: 43 Score: 5.0\n",
            "Epoch: 44 Score: 0.0\n",
            "Epoch: 45 Score: 0.0\n",
            "Epoch: 46 Score: 5.0\n",
            "Epoch: 47 Score: 1.0\n",
            "Epoch: 48 Score: 0.0\n",
            "Epoch: 49 Score: 0.0\n",
            "Epoch: 50 Score: 4.0\n",
            "Epoch: 51 Score: 3.0\n",
            "Epoch: 52 Score: 5.0\n",
            "Epoch: 53 Score: 0.0\n",
            "Epoch: 54 Score: 0.0\n",
            "Epoch: 55 Score: 0.0\n",
            "Epoch: 56 Score: 4.0\n",
            "Epoch: 57 Score: 0.0\n",
            "Epoch: 58 Score: 2.0\n",
            "Epoch: 59 Score: 2.0\n",
            "Epoch: 60 Score: 2.0\n",
            "Epoch: 61 Score: 2.0\n",
            "Epoch: 62 Score: 0.0\n",
            "Epoch: 63 Score: 0.0\n",
            "Epoch: 64 Score: 3.0\n",
            "Epoch: 65 Score: 3.0\n",
            "Epoch: 66 Score: 1.0\n",
            "Epoch: 67 Score: 5.0\n",
            "Epoch: 68 Score: 5.0\n",
            "Epoch: 69 Score: 0.0\n",
            "Epoch: 70 Score: 0.0\n",
            "Epoch: 71 Score: 0.0\n",
            "Epoch: 72 Score: 5.0\n",
            "Epoch: 73 Score: 5.0\n",
            "Epoch: 74 Score: 5.0\n",
            "Epoch: 75 Score: 1.0\n",
            "Epoch: 76 Score: 0.0\n",
            "Epoch: 77 Score: 5.0\n",
            "Epoch: 78 Score: 0.0\n",
            "Epoch: 79 Score: 2.0\n",
            "Epoch: 80 Score: 0.0\n",
            "Epoch: 81 Score: 0.0\n",
            "Epoch: 82 Score: 0.0\n",
            "Epoch: 83 Score: 2.0\n",
            "Epoch: 84 Score: 0.0\n",
            "Epoch: 85 Score: 0.0\n",
            "Epoch: 86 Score: 0.0\n",
            "Epoch: 87 Score: 0.0\n",
            "Epoch: 88 Score: 1.0\n",
            "Epoch: 89 Score: 0.0\n",
            "Epoch: 90 Score: 0.0\n",
            "Epoch: 91 Score: 0.0\n",
            "Epoch: 92 Score: 0.0\n",
            "Epoch: 93 Score: 0.0\n",
            "Epoch: 94 Score: 5.0\n",
            "Epoch: 95 Score: 5.0\n",
            "Epoch: 96 Score: 0.0\n",
            "Epoch: 97 Score: 5.0\n",
            "Epoch: 98 Score: 0.0\n",
            "Epoch: 99 Score: 0.0\n",
            "Epoch: 100 Score: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU7wzMg67-us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = wrap_env(gym.make('BreakoutDeterministic-v4'))\n",
        "state = env.reset()\n",
        "is_done = False\n",
        "states = deque(maxlen = 4)\n",
        "for time_t in range(1000):\n",
        "  env.render()\n",
        "  if len(states)<4:\n",
        "    action = env.action_space.sample()\n",
        "  else:\n",
        "    not_states = np.reshape(np.array(start_states),[1,105,80,4])\n",
        "    preds = model.predict([not_states, np.ones([1,4])])\n",
        "    action = np.argmax(preds)\n",
        "  state = preprocess(state)\n",
        "  states.append(state)\n",
        "  state, reward, is_done, _ = env.step(action)\n",
        "  if is_done:\n",
        "    break\n",
        "\n",
        "env.close()\n",
        "show_video()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}